{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: Relation extraction using distant supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Bill MacCartney and Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Fall 2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Baselines](#Baselines)\n",
    "  1. [Hand-build feature functions](#Hand-build-feature-functions)\n",
    "  1. [Distributed representations](#Distributed-representations)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Different model factory [1 points]](#Different-model-factory-[1-points])\n",
    "  1. [Directional unigram features [1.5 points]](#Directional-unigram-features-[1.5-points])\n",
    "  1. [The part-of-speech tags of the \"middle\" words [1.5 points]](#The-part-of-speech-tags-of-the-\"middle\"-words-[1.5-points])\n",
    "  1. [Bag of Synsets [2 points]](#Bag-of-Synsets-[2-points])\n",
    "  1. [Your original system [3 points]](#Your-original-system-[3-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bake-off are devoted to developing really effective relation extraction systems using distant supervision. \n",
    "\n",
    "As with the previous assignments, this notebook first establishes a baseline system. The initial homework questions ask you to create additional baselines and suggest areas for innovation, and the final homework question asks you to develop an original system for you to enter into the bake-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](rel_ext_01_task.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import os\n",
    "import rel_ext\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import utils\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.ensemble import RandomForestClassifier,StackingClassifier,VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we unite our corpus and KB into a dataset, and create some splits for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/teja/relation_extraction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_ext_data_home = os.getcwd()\n",
    "rel_ext_data_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'corpus.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'kb.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_HOME = os.path.join(os.getcwd(), 'glove.6B')\n",
    "\n",
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join('/home/teja', 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not wedded to this set-up for splits. The bake-off will be conducted on a previously unseen test-set, so all of the data in `dataset` is fair game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['tiny', 'train', 'dev'],\n",
    "    split_fracs=[0.01, 0.79, 0.20],\n",
    "    seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny': Corpus with 3,474 examples; KB with 445 triples,\n",
       " 'train': Corpus with 263,285 examples; KB with 36,191 triples,\n",
       " 'dev': Corpus with 64,937 examples; KB with 9,248 triples,\n",
       " 'all': Corpus with 331,696 examples; KB with 45,884 triples}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-build feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in get_tag_bigrams(ex.middle_POS):\n",
    "            feature_counter[word] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in get_tag_bigrams(ex.middle_POS):\n",
    "            feature_counter[word] += 1\n",
    "    return feature_counter\n",
    "\n",
    "\n",
    "def get_tag_bigrams(s):\n",
    "    \"\"\"Suggested helper method for `middle_bigram_pos_tag_featurizer`.\n",
    "    This should be defined so that it returns a list of str, where each\n",
    "    element is a POS bigram.\"\"\"\n",
    "    # The values of `start_symbol` and `end_symbol` are defined\n",
    "    # here so that you can use `test_middle_bigram_pos_tag_featurizer`.\n",
    "    start_symbol = \"<s>\"\n",
    "    end_symbol = \"</s>\"\n",
    "    tags=get_tags(s)\n",
    "    tags=[start_symbol]+tags+[end_symbol]\n",
    "    toks=bigrams(tags)\n",
    "    toks=[\" \".join(x) for x in toks]\n",
    "    toks.extend(tags)\n",
    "    return toks\n",
    "\n",
    "def get_tags(s):\n",
    "    \"\"\"Given a sequence of word/POS elements (lemmas), this function\n",
    "    returns a list containing just the POS elements, in order.\n",
    "    \"\"\"\n",
    "    return [parse_lem(lem)[1] for lem in s.strip().split(' ') if lem]\n",
    "\n",
    "\n",
    "def parse_lem(lem):\n",
    "    \"\"\"Helper method for parsing word/POS elements. It just splits\n",
    "    on the rightmost / and returns (word, POS) as a tuple of str.\"\"\"\n",
    "    return lem.strip().rsplit('/', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBeforePOS(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in getBeforetokensPOS(ex.left_POS):\n",
    "            feature_counter[word+\"_before\"] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in getBeforetokensPOS(ex.left_POS):\n",
    "            feature_counter[word+\"_before\"] += 1\n",
    "    return feature_counter\n",
    "\n",
    "def getAfterPOS(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in getAftertokensPOS(ex.right_POS):\n",
    "            feature_counter[word+\"_after\"] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in getAftertokensPOS(ex.right_POS):\n",
    "            feature_counter[word+\"_after\"] += 1\n",
    "    return feature_counter\n",
    "\n",
    "def getBeforetokensPOS(x):\n",
    "    tokens=x.split(\" \")[-3:]\n",
    "    if len(tokens)==0:\n",
    "        return [\"NOPOS\"]\n",
    "    try:\n",
    "        pos_tags=[x.split(\"/\")[1] for x in tokens]\n",
    "    except Exception as e:\n",
    "        return [\"NOPOS\"]\n",
    "    return pos_tags\n",
    "def getAftertokensPOS(x):\n",
    "    tokens=x.split(\" \")[0:3]\n",
    "    if len(tokens)==0:\n",
    "        return [\"NOPOS\"]\n",
    "    try:\n",
    "        pos_tags=[x.split(\"/\")[1] for x in tokens]\n",
    "    except Exception as e:\n",
    "        return [\"NOPOS\"]\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word+\"_SO\"] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word+\"_OS\"] += 1\n",
    "    return feature_counter\n",
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    return feature_counter\n",
    "\n",
    "def left_bag_of_words(kbt,corpus,feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.left.split(' '):\n",
    "            feature_counter[word+\"_left\"] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.left.split(' '):\n",
    "            feature_counter[word+\"_left\"] += 1\n",
    "    return feature_counter\n",
    "def right_bag_of_words(kbt,corpus,feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.right.split(' '):\n",
    "            feature_counter[word+\"_right\"] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.right.split(' '):\n",
    "            feature_counter[word+\"_right\"] += 1\n",
    "    return feature_counter\n",
    "\n",
    "def SorO(kbt,corpus,feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        feature_counter[\"subject\"] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        feature_counter[\"object\"] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synsets(s):\n",
    "    \"\"\"Suggested helper method for `synset_featurizer`. This should\n",
    "    be completed so that it returns a list of stringified Synsets\n",
    "    associated with elements of `s`.\n",
    "    \"\"\"\n",
    "    # Use `parse_lem` from the previous question to get a list of\n",
    "    # (word, POS) pairs. Remember to convert the POS strings.\n",
    "    wt = [parse_lem(lem) for lem in s.strip().split(' ') if lem]\n",
    "    wt=[[x[0],convert_tag(x[1])] for x in wt]\n",
    "    sysnets=[]\n",
    "    for i in wt:\n",
    "        nets=wn.synsets(i[0], pos=i[1])\n",
    "        nets=list(map(str,nets))\n",
    "        sysnets.extend(nets)\n",
    "    return sysnets\n",
    "\n",
    "\n",
    "def synset_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in get_synsets(ex.middle_POS):\n",
    "            feature_counter[word] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in get_synsets(ex.middle_POS):\n",
    "            feature_counter[word] += 1\n",
    "    return feature_counter\n",
    "\n",
    "\n",
    "def convert_tag(t):\n",
    "    \"\"\"Converts tags so that they can be used by WordNet:\n",
    "\n",
    "    | Tag begins with | WordNet tag |\n",
    "    |-----------------|-------------|\n",
    "    | `N`             | `n`         |\n",
    "    | `V`             | `v`         |\n",
    "    | `J`             | `a`         |\n",
    "    | `R`             | `r`         |\n",
    "    | Otherwise       | `None`      |\n",
    "    \"\"\"\n",
    "    if t[0].lower() in {'n', 'v', 'r'}:\n",
    "        return t[0].lower()\n",
    "    elif t[0].lower() == 'j':\n",
    "        return 'a'\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_middle_featurizer(kbt, corpus, np_func=np.sum):\n",
    "    reps = []\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split():\n",
    "            rep = glove_lookup.get(word)\n",
    "            if rep is not None:\n",
    "                reps.append(rep)\n",
    "    if len(reps) == 0:\n",
    "        dim = len(next(iter(glove_lookup.values())))\n",
    "        return utils.randvec(n=dim)\n",
    "    else:\n",
    "        return np_func(reps, axis=0)\n",
    "def glove_left_featurizer(kbt, corpus, np_func=np.sum):\n",
    "    reps = []\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.left.split()[-3:]:\n",
    "            rep = glove_lookup.get(word)\n",
    "            if rep is not None:\n",
    "                reps.append(rep)\n",
    "    if len(reps) == 0:\n",
    "        dim = len(next(iter(glove_lookup.values())))\n",
    "        return utils.randvec(n=dim)\n",
    "    else:\n",
    "        return np_func(reps, axis=0)\n",
    "    \n",
    "def glove_right_featurizer(kbt, corpus, np_func=np.sum):\n",
    "    reps = []\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.right.split()[0:3]:\n",
    "            rep = glove_lookup.get(word)\n",
    "            if rep is not None:\n",
    "                reps.append(rep)\n",
    "    if len(reps) == 0:\n",
    "        dim = len(next(iter(glove_lookup.values())))\n",
    "        return utils.randvec(n=dim)\n",
    "    else:\n",
    "        return np_func(reps, axis=0)\n",
    "    \n",
    "def combineGloveFeatures(kbt, corpus, np_func=np.sum):\n",
    "    left_features=glove_left_featurizer(kbt, corpus)\n",
    "    middle_features=glove_middle_featurizer(kbt, corpus) \n",
    "    right_features=glove_right_featurizer(kbt, corpus)\n",
    "    final_features=np.concatenate([left_features,middle_features,right_features],0)\n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizers = [simple_bag_of_words_featurizer,left_bag_of_words,right_bag_of_words,SorO,directional_bag_of_words_featurizer,middle_bigram_pos_tag_featurizer,getBeforePOS,getAfterPOS,synset_featurizer,combineGloveFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_factory = lambda: LogisticRegression(fit_intercept=True, solver='liblinear',n_jobs=-1)\n",
    "# model_factory = lambda: RandomForestClassifier(n_estimators=100,random_state=42,\n",
    "#                                                class_weight=\"balanced_subsample\",n_jobs=-1)\n",
    "\n",
    "estimators=[('lr',LogisticRegression(fit_intercept=True, solver='liblinear',n_jobs=-1,random_state=42)),\n",
    "           ('rf',RandomForestClassifier(n_estimators=100,random_state=42,class_weight=\"balanced_subsample\",n_jobs=-1))]\n",
    "\n",
    "model_factory = lambda: StackingClassifier(estimators=estimators,n_jobs=-1,\n",
    "                                           final_estimator=LogisticRegression(fit_intercept=True, solver='liblinear',n_jobs=-1,random_state=42),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lr',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                     multi_class='auto', n_jobs=-1, penalty='l2', random_state=42,\n",
       "                     solver='liblinear', tol=0.0001, verbose=0, warm_start=False)),\n",
       " ('rf',\n",
       "  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                         class_weight='balanced_subsample', criterion='gini',\n",
       "                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                         max_samples=None, min_impurity_decrease=0.0,\n",
       "                         min_impurity_split=None, min_samples_leaf=1,\n",
       "                         min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                         n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "                         random_state=42, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hurray completed all the assertions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "  6%|▋         | 1/16 [03:50<57:39, 230.61s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 12%|█▎        | 2/16 [07:35<53:23, 228.81s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 19%|█▉        | 3/16 [11:20<49:19, 227.68s/it]/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 25%|██▌       | 4/16 [19:01<59:32, 297.73s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 31%|███▏      | 5/16 [23:06<51:41, 281.96s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 38%|███▊      | 6/16 [27:12<45:11, 271.13s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 44%|████▍     | 7/16 [29:16<34:01, 226.85s/it]/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 50%|█████     | 8/16 [34:28<33:40, 252.50s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 56%|█████▋    | 9/16 [40:15<32:45, 280.82s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 62%|██████▎   | 10/16 [44:52<27:57, 279.64s/it]/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 69%|██████▉   | 11/16 [49:23<23:05, 277.12s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 75%|███████▌  | 12/16 [52:15<16:22, 245.57s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 81%|████████▏ | 13/16 [55:26<11:28, 229.34s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 88%|████████▊ | 14/16 [58:58<07:27, 223.95s/it]/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      " 94%|█████████▍| 15/16 [1:03:10<03:52, 232.35s/it]/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "100%|██████████| 16/16 [1:07:14<00:00, 252.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hurray completed all the assertions\n",
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.920      0.471      0.772        340       5716\n",
      "author                    0.899      0.631      0.829        509       5885\n",
      "capital                   0.778      0.221      0.517         95       5471\n",
      "contains                  0.896      0.733      0.858       3904       9280\n",
      "film_performance          0.910      0.661      0.846        766       6142\n",
      "founders                  0.890      0.342      0.674        380       5756\n",
      "genre                     0.852      0.306      0.628        170       5546\n",
      "has_sibling               0.898      0.513      0.781        499       5875\n",
      "has_spouse                0.908      0.466      0.764        594       5970\n",
      "is_a                      0.904      0.302      0.646        497       5873\n",
      "nationality               0.851      0.322      0.641        301       5677\n",
      "parents                   0.847      0.497      0.742        312       5688\n",
      "place_of_birth            0.831      0.296      0.611        233       5609\n",
      "place_of_death            0.750      0.151      0.418        159       5535\n",
      "profession                0.917      0.312      0.660        247       5623\n",
      "worked_at                 0.862      0.310      0.636        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.870      0.408      0.689       9248      95264\n"
     ]
    }
   ],
   "source": [
    "baseline_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=model_factory,\n",
    "    train_sampling_rate=0.2,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying model weights might yield insights:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
